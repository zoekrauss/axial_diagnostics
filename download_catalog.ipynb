{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in earthquake catalog from Axial Seamount website HYPO71 files for the past year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = datetime(2019,2,20)\n",
    "t2 = datetime(2019,3,7)\n",
    "day_list = pd.date_range(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'http://axial.ocean.washington.edu/hypo71/hypo71_'\n",
    "url_list = [url_base+d.strftime('%Y%m%d')+'.dat' for d in day_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://axial.ocean.washington.edu/hypo71/hypo71_20190307.dat\r"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "lats = []\n",
    "lons = []\n",
    "PMom = []\n",
    "SMom = []\n",
    "depths = []\n",
    "ids = []\n",
    "for url in url_list:\n",
    "    print(url,end='\\r')\n",
    "    try:\n",
    "        data = urllib.request.urlopen(url)\n",
    "        lines = []\n",
    "        for line in data:\n",
    "            lines.append(str(line))\n",
    "        for entry in lines[1:]:\n",
    "            entry_split = entry[1:].split(' ')\n",
    "            entry_time_string = entry_split[0][1:]+entry_split[1]+entry_split[2]\n",
    "            if entry_split[2]=='':\n",
    "                entry_time_string = entry_split[0][1:]+entry_split[1]+entry_split[3]\n",
    "            date_format = '%Y%m%d%H%M%S.%f'\n",
    "            date_obj = datetime.strptime(entry_time_string, date_format)\n",
    "            times.append(date_obj)\n",
    "    except:\n",
    "        print('Failed for '+url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "with open('earthquake_times_2019', 'wb') as f:\n",
    "    # dump information to that file\n",
    "    pickle.dump(times, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alaska-ml",
   "language": "python",
   "name": "alaska-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
